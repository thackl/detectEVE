# relative to workflow dir
wf_dir=config["wf_dir"]
configfile: f"{wf_dir}/config.yaml"

import pandas as pd
import glob
import requests
import re
import os
from os.path import exists

scripts=os.path.join(wf_dir, "workflow/scripts")

dbs=config["db_dir"]
if not os.path.isabs(dbs):
    dbs=os.path.join(wf_dir, dbs)

# read accessions for genome download
def acc():
    tsv_acc = []
    if(os.path.exists("accessions.csv")):
        df = pd.read_csv("accessions.csv", sep=",")
        tsv_acc = df.iloc[:, 0].tolist()

    fna_acc = glob.glob("genomes/*.fna")
    fna_acc = [x.removeprefix("genomes/").removesuffix(".fna") for x in fna_acc]

    return list(set(tsv_acc + fna_acc))

if not config["mask"]["db"]:
    ruleorder: skip_mask_putatEVEs > mask_putatEVEs
else:
    ruleorder: mask_putatEVEs > skip_mask_putatEVEs

# toggle download of preclustered RVDB vs local clustering
if not config["setup"]["rvdb-custom-clustering"]:
    ruleorder: unpack_rvdb_clustered > cluster_rvdb
else:
    ruleorder: cluster_rvdb > unpack_rvdb_clustered


## Pseudo-rules
rule all:
    input:
        expand("results/{asm}-validatEVEs.{ext}", asm=acc(), ext=["fna", "pdf", "tsv"])

rule setup:
    input:
        f"{dbs}/uniref50.dmnd",
        f"{dbs}/rvdb-clustered.dmnd"
    
## genome download ----------------------------------------------------------##
rule download_genome:
    output:
        fna=temp("genomes/{asm}.fna"),
        htm=temp("genomes/{asm}.htm")
    log: "genomes/{asm}.log"
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    # I tried ftp before but not all accessions seem to be there (e.g. Cafeteria
    # VLTM01, ...)  Multiple links are split genomes, and should be concatenated
    # (e.g. NIUR01.1.fsa_nt.gz, NIUR01.2.fsa_nt.gz, ...)
    shell:
        "{scripts}/download-traces.sh {wildcards.asm} {output.htm} > {output.fna} 2> {log}"

## analysis -----------------------------------------------------------------##
rule search_assemblies:
    input: "genomes/{asm}.fna"
    output:
        tmp="results/{asm}-search.o6",
        bed="results/{asm}-search.bed"
    threads: workflow.cores
    shell:
        "{scripts}/diamond-chopped.sh blastx"
        " --query {input} --out {output.tmp} --threads {threads}"
        " --db {dbs}/{config[search][db]} {config[search][taxonlist]}"
        " --evalue {config[search][evalue]} {config[search][other_args]}"
        " -W {config[search][chop_window]} -S {config[search][chop_step]}\n"
        "{scripts}/blast2bed -qa {output.tmp} |"
        " {scripts}/bed-top -m {config[search][min_length_nt]} > {output.bed}"

rule extract_putatEVEs:
    input:
        bed="results/{asm}-search.bed",
        fna="genomes/{asm}.fna"
    output: 
        fai="genomes/{asm}.fna.seqkit.fai",
        fna="results/{asm}-putatEVEs-unmasked.fna"
    shell:
        """
        seqkit subseq --bed {input} > {output.fna}
        if ! [ -s {output.fna} ] || ! [ -s {output.fai} ]; then
          echo ""
          echo ""
          echo "#--------------------------------------------------------------------------------------#"
          echo "NOTE: No putatEVEs detected for [{wildcards.asm}] with current DB/sensitivity settings;"
          echo "#--------------------------------------------------------------------------------------#"
          echo ""
          exit 1
        fi;
        """

rule masksearch_putatEVEs:
    input: "results/{asm}-putatEVEs-unmasked.fna" 
    output:
        o6="results/{asm}-mask.o6",
        bed="results/{asm}-mask.bed"
    threads: workflow.cores
    shell:
        "diamond blastx --query {input} --threads {threads}"
        " --db {dbs}/{config[mask][db]}"
        " --evalue {config[search][evalue]}"
        " {config[search][other_args]}"
        " --out {output.o6}\n"
        "{scripts}/blast2bed -qa {output.o6} |"
        " {scripts}/bed-top -m {config[search][min_length_nt]} > {output.bed}"

rule mask_putatEVEs:
    input:
        fna="results/{asm}-putatEVEs-unmasked.fna",
        bed="results/{asm}-mask.bed"
    output: "results/{asm}-putatEVEs.fna"
    shell:
        "bedtools maskfasta -fi {input.fna} -bed {input.bed} -fo {output} -fullHeader"

rule skip_mask_putatEVEs:
    input: "results/{asm}-putatEVEs-unmasked.fna" 
    output: "results/{asm}-putatEVEs.fna"
    shell: "mv {input} {output}"
        
rule retrosearch_putatEVEs_udb:
    input: "results/{asm}-putatEVEs.fna" 
    output: "results/{asm}-retro-udb.o6",
    threads: workflow.cores
    shell:
        "diamond blastx --query {input} --threads {threads}"
        " --db {dbs}/{config[retrosearch][db]}"
        " --evalue {config[retrosearch][evalue]}"
        " {config[retrosearch][other_args]}"
        " --out {output}\n"

rule retrosearch_putatEVEs_vdb:
    input: "results/{asm}-putatEVEs.fna" 
    output: "results/{asm}-retro-vdb.o6",
    threads: workflow.cores
    shell:
        "diamond blastx --query {input} --threads {threads}"
        " --db {dbs}/{config[search][db]}"
        " --evalue {config[search][evalue]}"
        " {config[search][other_args]}"
        " --out {output}\n"

rule retrosearch_putatEVEs_merge:
    input:
        udb="results/{asm}-retro-udb.o6",
        vdb="results/{asm}-retro-vdb.o6",
    output:
        bed=temp("results/{asm}-retro-notax.bed")
    threads: workflow.cores
    shell:
        "(sed 's/$/\tVDB/' {input.vdb}; sed 's/$/\tUDB/' {input.udb}) |"
        " {scripts}/blast2bed -qa | {scripts}/bed-top -tk 20 > {output.bed}"

rule taxize_putatEVEs:
    input: "results/{asm}-retro-notax.bed"
    output: "results/{asm}-retro.bed"
    threads: workflow.cores
    shell:
        # parse taxid to column, get lineage
        r"""
        perl -pe '($t) = /taxid[ :=](\d+)/i; $t//="1"; s/$/\t$t/;' {input} |
        taxonkit reformat --data-dir {dbs} -P -I 19 -a -F -f "{{k}};{{K}};{{p}};{{c}};{{o}};{{f}};{{g}};{{s}}" > {output};
        """
        
rule validate_putatEVEs:
    input:
        bed="results/{asm}-retro.bed",
        fai="genomes/{asm}.fna.seqkit.fai"
    output:
        tsv="results/{asm}-validatEVEs.tsv",
        pdf="results/{asm}-validatEVEs.pdf"
    params: "{asm}"
    shell:
        """
        Rscript --vanilla {scripts}/validate.R -p {params} {config[validate][args]} {input} {output.tsv} {output.pdf}
        if ! [ -s {output.tsv} ]; then
          echo ""
          echo ""
          echo "#--------------------------------------------------------------------------------------#"
          echo "NOTE: No validatEVEs detected for [{wildcards.asm}] with current validation criteria;"
          echo "#--------------------------------------------------------------------------------------#"
          echo ""
          exit 1
        fi;
        """

rule extract_validatEVEs:
    input:
        fna="results/{asm}-putatEVEs.fna",
        tsv="results/{asm}-validatEVEs.tsv"
    output: "results/{asm}-validatEVEs.fna" 
    params: "{asm}"
    shell:
        "cut -f6 {input.tsv} | sed '1d' | "
        " seqkit faidx {input.fna} --infile-list - -f |"
        " perl -pe 's/^>/sprintf(\">%s_EVE%03d \", {params}, ++$i)/e' > {output}"





## databases downloads ------------------------------------------------------------##
rule download_ncbi_taxonomy:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output:
        targz=temp("{dbs}/taxdump.tar.gz"),
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp",
        delno="{dbs}/delnodes.dmp",
        merge="{dbs}/merged.dmp"
    shell:
        "curl https://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz -o {output.targz}\n"
        "cd {dbs}/ && tar -xzf taxdump.tar.gz nodes.dmp names.dmp delnodes.dmp merged.dmp"

rule download_ncbi_acc2tax:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: temp("{dbs}/prot.accession2taxid.FULL.gz")
    shell: "curl https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.FULL.gz -o {output}"

      
rule download_uniref:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: temp("{dbs}/uniref50.fasta.gz")
    shell:
        "curl https://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref50/uniref50.fasta.gz -o {output}"
         

rule download_rvdb:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: temp("{dbs}/rvdb100.faa.xz"),  # do not mark temp(), could be userspecified location
    shell:
        """
        set +o pipefail;
        url=https://rvdb-prot.pasteur.fr/
        db=$(curl -fs $url | grep -oPm1 'files/U-RVDBv[0-9.]+-prot.fasta.xz')
        curl $url/$db -o {output}
        """

rule download_rvdb_clustered:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores * 0.60
    output: temp("{dbs}/rvdb-clustered.faa.xz"),  # do not mark temp(), could be userspecified location
    shell:
        """
        set +o pipefail;
        url=https://rvdb-prot.pasteur.fr/
        db=$(curl -fs $url | grep -oPm1 'files/U-RVDBv[0-9.]+-prot_clustered.fasta.xz')
        curl $url/$db -o {output}
        """

## DEPRECATED - work with NR
# rule download_nr:
#     # a bit dirty to prevent mulitple downloads to run at same time
#     threads: workflow.cores * 0.60
#     output: temp("{dbs}/nr.faa.gz")
#     shell: "curl https://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz -o nr.faa.gz"

## databases prep ------------------------------------------------------------##
rule setup_uniref:
    input:
        faagz="{dbs}/uniref50.fasta.gz", # do not mark temp(), could be userspecified location
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        taxonmap=temp("{dbs}/uniref50-acc2tax.tsv.gz"),
        dmnd="{dbs}/uniref50.dmnd"
    shell:
        r"""
        ( echo -e "accession.version\ttaxid";
          gzip -cd {input.faagz} |
          perl -ne 'if(/>UniRef50_(\S+).*TaxID=(\d+)/){{print $1,"\t",$2,"\n"}}'
        ) | gzip -c > {output.taxonmap}
        """
        'diamond makedb --in {input.faagz} --db {output.dmnd}'
        ' --taxonmap {output.taxonmap} --taxonnodes {input.nodes} --taxonnames {input.names}'

rule unpack_rvdb:
    # a bit dirty to prevent mulitple downloads to run at same time
    input:
        xz="{dbs}/rvdb100.faa.xz",
    output:
        faa=temp("{dbs}/rvdb100.faa")
    shell:
        r"""
        # unpack
        xz -cd {input.xz} | seqkit seq -m 60 - | seqkit rmdup -s - | perl -pe 's/^>(acc\|\w+\|)([^|]+)/>$2 $1$2/' > {output.faa}
        """

rule unpack_rvdb_clustered:
    # a bit dirty to prevent mulitple downloads to run at same time
    input:
        xz="{dbs}/rvdb-clustered.faa.xz",
    output:
        faa=temp("{dbs}/rvdb-clustered-notax.faa")
    shell:
        r"""
        # unpack
        xz -cd {input.xz} | seqkit seq -m 60 - | seqkit rmdup -s - | perl -pe 's/^>(acc\|\w+\|)([^|]+)/>$2 $1$2/' > {output.faa}
        """

# 16GB RAM should suffice: "Approximated maximum memory consumption: 10179M"
rule cluster_rvdb:
    input:
        "{dbs}/rvdb100.faa"
    output:
        faa=temp("{dbs}/rvdb-clustered-notax.faa"),
        cls=temp("{dbs}/rvdb-clustered-notax.faa.clstr"),
    threads: workflow.cores
    shell:
        """
        cd-hit {config[setup][rvdb-custom-clustering]} -i {input} -o {output.faa} -T {threads}
        """

rule prep_rvdb_taxonomy:
    # a bit dirty to prevent mulitple downloads to run at same time
    threads: workflow.cores
    input:
        faa="{dbs}/rvdb-clustered-notax.faa",
        mapgz="{dbs}/prot.accession2taxid.FULL.gz"
    output:
        acc=temp("{dbs}/rvdb-clustered-acc.tsv"),
        tax=temp("{dbs}/rvdb-clustered-acc2tax.tsv"),
        faa=temp("{dbs}/rvdb-clustered.faa"),
        taxgz=temp("{dbs}/rvdb-clustered-acc2tax.tsv.gz"),
        faagz=temp("{dbs}/rvdb-clustered.faa.gz")
    shell:
        r"""
        grep '^>' {input.faa} | cut -f3 -d'|' > {output.acc} 
        csvtk --quiet -tT grep -j {threads} -P {output.acc} {input.mapgz} > {output.tax}
        seqkit replace -p 'acc\|[^\|]+\|([^\|]+).*' -r '${{0}} acc=${{1}} taxid={{kv}}' -k {output.tax} {input.faa} -m NA > {output.faa}
        gzip -k {output.tax}
        gzip -k {output.faa}
        """

rule setup_rvdb:
    input:
        faagz="{dbs}/rvdb-clustered.faa.gz",
        mapgz="{dbs}/rvdb-clustered-acc2tax.tsv.gz",
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        dmnd="{dbs}/rvdb-clustered.dmnd"
    shell:
        """
        diamond makedb --no-parse-seqids --in {input.faagz} --db {output.dmnd} --taxonmap {input.mapgz} --taxonnodes {input.nodes} --taxonnames {input.names}
        """

rule setup_retrodb:
    input:
        rvdb_faagz="{dbs}/rvdb-clustered.faa.gz",
        rvdb_taxgz="{dbs}/rvdb-clustered-acc2tax.tsv.gz",
        uniref_faagz="{dbs}/uniref50.fasta.gz",
        uniref_taxgz="{dbs}/uniref50-acc2tax.tsv.gz",
        nodes="{dbs}/nodes.dmp",
        names="{dbs}/names.dmp"
    output:
        taxgz=temp("{dbs}/rvdb-clustered-uniref50-acc2tax.tsv.gz"),
        dmnd="{dbs}/rvdb-clustered-uniref50.dmnd"
    shell:
        """
        cp {input.rvdb_taxgz} {output.taxgz}
        zcat {input.uniref_taxgz} | sed -n '2,$p' | gzip -c >> {output.taxgz}
        zcat {input.rvdb_faagz} {input.uniref_faagz} |
        diamond makedb --db {output.dmnd} --taxonmap {output.taxgz} --taxonnodes {input.nodes} --taxonnames {input.names}
        """

## DEPRECATED
# rule setup_nr:
#     input:
#         faagz="{dbs}/nr.faa.gz",
#         mapgz="{dbs}/prot.accession2taxid.FULL.gz",
#         nodes="{dbs}/nodes.dmp",
#         names="{dbs}/names.dmp",
#     output:
#         dmnd="{dbs}/nr.dmnd"
#     shell:
#         'diamond makedb --in {input.faagz} --db {output.dmnd} --taxonmap {input.mapgz} --taxonnodes {input.nodes} --taxonnames {input.names}'
